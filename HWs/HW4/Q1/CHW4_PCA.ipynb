{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Computer Homework 4\n",
    "## Principal Component Analysis (PCA)\n",
    "In this question you will implement the PCA from scratch and apply it for dimensionality\n",
    "reduction and image denoising. you will use the well-known MNIST data set that is commonly used for training various image processing systems.\n",
    "\n",
    "You are only required to write code for TODO parts with given descriptions.\n",
    "\n",
    "Feel free to contact me via telegram if you have any question: @mtv_tavana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "# you can use any library which is needed for your approaches.\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation\n",
    "The MNIST data set is a large data set of handwritten digits (from\n",
    "0 to 9), containing 60000 gray-scale images for training and 10000 for testing, each image has 28x28 pixels with range of possible values from 0 to 255.\n",
    "\n",
    "we will consider a small sample of MNIST data set with size of 2000 which is attached to your homework zip file as \"MNIST_DATASET_PNG\" folder.\n",
    "\n",
    "Re-scale the images to [0, 1] dividing them by 255. Vectorize each image xi ∈ $R^d$ and form a matrix X=$[x1, . . . , xn]^T$ ∈ $R_{n×d}$.\n",
    "Remark that we will have d=784 and n=2000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the data set and Re-scale the images to [0, 1] dividing them by 255.\n",
    "# TODO: vectorize each image and forming matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the Eigendecomposition\n",
    "Compute the eigendecomposition of the sample covariance matrix and use the eigenvalues to calculate the percentage of variance explained (given by the eigenvalues).\n",
    "Plot the cumulative sum of these percentages (also known as cumulative explained variance. you can read about it in [cumulative explained variance](https://vitalflux.com/pca-explained-variance-concept-python-example/)) versus the number of components.\n",
    "\n",
    "*Hint: you can use cumsum from Numpy to calculate the cumulative sum.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the eigendecomposition\n",
    "# TODO: Plot the cumulative explained variance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce the Dimensionality\n",
    "Apply the PCA via Eigendecomposition to reduce the dimensionality of the images for each p ∈ {50, 250, 500}.\n",
    "\n",
    "Compute the normalized reconstruction error in terms of the Frobenius norm, i.e. $e_p = \\frac{||X - \\^{X_p}||_F}{||X||_F}$ , where $X$ denotes the input matrix, and $\\^{X_p}$ denotes the recovered matrix associated to each p. \n",
    "\n",
    "Visualize some recovered images and compare them with their corresponding original images.\n",
    "\n",
    "what happens when we reduce the number of components p?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the PCA\n",
    "# TODO: Compute the normalized reconstruction error\n",
    "# TODO: Visualize recovered images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Image noising\n",
    "Considering the same input matrix, let’s add some Gaussian noise (make sure that the range of possible values of the generated noisy data will remain between 0 and1, as well as the input data) with zero mean and variance $σ^2$ = 0.25.\n",
    "\n",
    "Visualize the corrupted images and compare them with their corresponding original images versus the number of components, as in the first item. \n",
    "\n",
    "Compare it with the one obtained in the noiseless case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adding Gaussian noise\n",
    "# TODO: Visualize the corrupted images\n",
    "# TODO: Comparing with noiseless case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Recovering Images\n",
    "Now we will apply the PCA for image denoising. Generate the noisy data, for each $σ^2$ ∈ {0.15, 0.25, 0.50}.\n",
    "Apply the PCA via Eigendecomposition for each $σ^2$ and fixing p = 250.\n",
    "\n",
    "Visualize some recovered images and compare them with their corresponding noisy images.\n",
    "\n",
    "Compute the normalized reconstruction error in terms of the Frobenius norm, obtained for all values of $σ^2$, with respect to the original images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generating noisy data\n",
    "# TODO: Apply PCA via Eigendecomposition\n",
    "# TODO: Visualize recovered images\n",
    "# TODO: compute the normalized reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e5df6957475ba0aae71d6e6c09437818e2a1d5d265301759705467e9d280c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
