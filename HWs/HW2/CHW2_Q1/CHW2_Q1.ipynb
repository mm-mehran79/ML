{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Computer Homework 2\n",
    "## 1: Polynomial Regression\n",
    "In this question, the dataset contains only one feature that is a real number. We want to learn polynomial regression from degree n=1 to degree n=15 using the linear learning method and determine the best degree n for polynomial by comparing the results. \n",
    "\n",
    "You are only required to write code in the sections marked with `TODO:`      \n",
    "Feel free to contact me via telegram if you have any question: @PouyaSha, @zahrasodagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Three datasets S, V and T have been provided to you. which are in train_data.npy, validation_data.npy and test_data.npy files respectively. As you can see, we use S, V and T respectively for training, choosing the best polynomial degree and error estimation for the best polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the training, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Based on the training data S, for each degree of polynomial n=1 to n=15, learn a polynomial with degree n by linear regression method. Call the learned polynomial for n $h_n$. Plot the empirical error of the polynomials $h_n$ for the training data S, which we denote by $L_S(h_n)$ in terms of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: learn the polynomial weights using numpy and plot the empirical errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In order to choose the best polynomial degree, we should not compare $L_S(h_n)$ for different (why?). We get an estimate of $L(h_n)$ based on the dataset V and call it $L_V(h_n)$ which is the average error for dataset V points.\n",
    "Calculate $L_V(h_n)$ for n=1,...,15 and plot it in terms of n, next to the $L_S(h_n)$ curve from the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO: YOUR ANSWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the errors on both training set and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare the behavior of $L_S(h_n)$ and $L_V(h_n)$ in terms of n and explain the difference between the two curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO: YOUR ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using the above results, conclude that the best regression of polynomials in this problem is of\n",
    "what degree? Which of the two curves $L_S(h_n)$ and $L_V(h_n)$ should be used for this conclusion?\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO: YOUR ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Call the best polynomial obtained in clause d as $h^*$. We donâ€™t know the true error value of this polynomial or $L(h^*)$ and again we have to settle for an estimate of it. Therefore, we consider the average error of $h^*$ on the dataset T or $L_T(h_n)$ as the estimate of $L(h^*)$. Calculate $L_T(h_n)$.\n",
    "\n",
    "    Can you intuitively explain why it is not good to use dataset V to estimate $L(h^*)$ and it is necessary to use a third dataset i.e. T?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO: YOUR ANSWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: report the error on testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus\n",
    "Complete the following functions and implement polynomial regression from scratch. Calculate the errors on all of the three sets and compare the results. What are the other methods that can be used to calculate the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_transform(X, degrees):\n",
    "    X_cp = X.copy()\n",
    "    for degree in degrees:\n",
    "        X =  # TODO append columns of higher degrees to X\n",
    "    return X\n",
    "\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    loss = # TODO Calculate the L2 loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    m = # TODO number of training examples\n",
    "    \n",
    "    dw = # TODO gradient of loss with respect to weights\n",
    "    db = # TODO gradient of loss with respect to bias\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def train(X, y, bs, degrees, epochs, lr):\n",
    "    m, n = # TODO shape of the input data\n",
    "    x = x_transform(X, degrees)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batches = # TODO list of indices for each batch that is generated randomly\n",
    "        for batch in batches:\n",
    "            xb = # TODO x batch\n",
    "            yb = # TODO y batch\n",
    "            \n",
    "            y_hat = # TODO calculate the hypothesis using w and b\n",
    "            \n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            w -= # TODO update w with respect to the learning rate\n",
    "            b -= # TODO update b with respect to the learning rate\n",
    "        \n",
    "        l = loss(y, np.dot(x, w) + b)\n",
    "        losses.append(l)\n",
    "        \n",
    "    # returning weights, bias and losses(List).\n",
    "    return w, b, losses\n",
    "\n",
    "\n",
    "def predict(X, w, b, degrees):\n",
    "    x1 = x_transform(X, degrees)\n",
    "    y = # TODO use w and b to get the predicted values\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fit the polynomials and evaluate the results using your own functions and \n",
    "# compare it to the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO: YOUR ANSWER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
